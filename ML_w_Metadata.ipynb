{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 19:37:35.218204: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-06 19:37:35.218256: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/joehuang/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# below is the code from the metadata_writer_for_image_classifier.py\n",
    "from tflite_support import flatbuffers\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
    "from tflite_support.task import vision\n",
    "from tflite_support.task import core\n",
    "from tflite_support.task import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a image: tsb/basedata/training/bad-image/2.jpg\n",
    "img = image.load_img('custom-ava-downloader/Training/training_bad_image/2.jpg')\n",
    "# save the image to the current directory\n",
    "img.save('2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[71, 73, 83],\n",
       "        [72, 74, 84],\n",
       "        [74, 76, 86],\n",
       "        ...,\n",
       "        [ 5,  7,  8],\n",
       "        [ 6,  8,  9],\n",
       "        [ 7,  9, 10]],\n",
       "\n",
       "       [[77, 79, 89],\n",
       "        [78, 80, 90],\n",
       "        [78, 80, 90],\n",
       "        ...,\n",
       "        [ 6,  8,  9],\n",
       "        [ 7,  9, 10],\n",
       "        [ 8, 10, 11]],\n",
       "\n",
       "       [[80, 82, 92],\n",
       "        [80, 82, 92],\n",
       "        [80, 82, 92],\n",
       "        ...,\n",
       "        [ 8, 10, 11],\n",
       "        [ 8, 10, 11],\n",
       "        [ 9, 11, 12]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cv2.imread('2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "validation = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1189 images belonging to 2 classes.\n",
      "Found 1165 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# every pictures have an assigned value\n",
    "train_dataset = train.flow_from_directory('custom-ava-downloader/Training',\n",
    "                                            target_size=(200,200),\n",
    "                                            batch_size= 64,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "validation_dataset = train.flow_from_directory('custom-ava-downloader/Validating',\n",
    "                                            target_size=(200,200),\n",
    "                                            batch_size=64,\n",
    "                                            class_mode='binary')\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_bad_image': 0, 'training_good_image': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_indices # {'bad-image': 0, 'good-image': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 198, 198, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 99, 99, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 97, 97, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 46, 46, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 33856)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               17334784  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,358,881\n",
      "Trainable params: 17,358,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joehuang/.local/lib/python3.8/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "8/8 - 28s - loss: 2.0008 - accuracy: 0.5820 - val_loss: 0.6497 - val_accuracy: 0.6504 - 28s/epoch - 4s/step\n",
      "Epoch 2/14\n",
      "8/8 - 26s - loss: 0.6380 - accuracy: 0.6825 - val_loss: 0.6404 - val_accuracy: 0.6660 - 26s/epoch - 3s/step\n",
      "Epoch 3/14\n",
      "8/8 - 25s - loss: 0.6260 - accuracy: 0.6777 - val_loss: 0.6538 - val_accuracy: 0.6426 - 25s/epoch - 3s/step\n",
      "Epoch 4/14\n",
      "8/8 - 24s - loss: 0.6099 - accuracy: 0.6758 - val_loss: 0.6319 - val_accuracy: 0.6934 - 24s/epoch - 3s/step\n",
      "Epoch 5/14\n",
      "8/8 - 26s - loss: 0.6514 - accuracy: 0.6094 - val_loss: 0.6550 - val_accuracy: 0.6543 - 26s/epoch - 3s/step\n",
      "Epoch 6/14\n",
      "8/8 - 27s - loss: 0.6016 - accuracy: 0.7093 - val_loss: 0.6744 - val_accuracy: 0.6426 - 27s/epoch - 3s/step\n",
      "Epoch 7/14\n",
      "8/8 - 23s - loss: 0.7513 - accuracy: 0.7320 - val_loss: 0.6682 - val_accuracy: 0.6641 - 23s/epoch - 3s/step\n",
      "Epoch 8/14\n",
      "8/8 - 25s - loss: 0.6242 - accuracy: 0.7361 - val_loss: 0.6883 - val_accuracy: 0.5957 - 25s/epoch - 3s/step\n",
      "Epoch 9/14\n",
      "8/8 - 31s - loss: 0.5562 - accuracy: 0.7402 - val_loss: 0.7784 - val_accuracy: 0.3945 - 31s/epoch - 4s/step\n",
      "Epoch 10/14\n",
      "8/8 - 24s - loss: 0.5364 - accuracy: 0.7441 - val_loss: 0.6477 - val_accuracy: 0.6777 - 24s/epoch - 3s/step\n",
      "Epoch 11/14\n",
      "8/8 - 29s - loss: 0.5652 - accuracy: 0.7285 - val_loss: 0.6356 - val_accuracy: 0.6816 - 29s/epoch - 4s/step\n",
      "Epoch 12/14\n",
      "8/8 - 25s - loss: 0.4733 - accuracy: 0.7656 - val_loss: 0.7731 - val_accuracy: 0.4766 - 25s/epoch - 3s/step\n",
      "Epoch 13/14\n",
      "8/8 - 23s - loss: 0.4301 - accuracy: 0.8247 - val_loss: 0.8264 - val_accuracy: 0.4727 - 23s/epoch - 3s/step\n",
      "Epoch 14/14\n",
      "8/8 - 23s - loss: 0.3990 - accuracy: 0.8301 - val_loss: 0.8382 - val_accuracy: 0.6621 - 23s/epoch - 3s/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\" All of the code below is for the model creation and training. \"\"\"\n",
    "# create a model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(200,200,3)), # 16 filters, 3x3 matrix\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2), # 2x2 matrix\n",
    "                                    tf.keras.layers.Conv2D(32, (3,3), activation='relu'), # 32 filters, 3x3 matrix\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2), # 2x2 matrix\n",
    "                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), # 64 filters, 3x3 matrix\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2), # 2x2 matrix\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'), # 512 neurons\n",
    "                                    tf.keras.layers.Dense(1, activation='sigmoid')]) # 1 neuron\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# metadata\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer=RMSprop(lr=0.001),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=8,\n",
    "                    epochs=14,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=8,\n",
    "                    verbose=2)\n",
    "\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmdm8qnko/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmdm8qnko/assets\n",
      "2022-12-06 19:51:23.722306: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-12-06 19:51:23.722349: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-12-06 19:51:23.722863: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpmdm8qnko\n",
      "2022-12-06 19:51:23.726162: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-12-06 19:51:23.726232: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpmdm8qnko\n",
      "2022-12-06 19:51:23.738192: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2022-12-06 19:51:23.835240: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpmdm8qnko\n",
      "2022-12-06 19:51:23.857078: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 134231 microseconds.\n",
      "/home/joehuang/.local/lib/python3.8/site-packages/tensorflow_lite_support/metadata/python/metadata.py:395: UserWarning: File, 'labels.txt', does not exist in the metadata. But packing it to tflite model is still allowed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" All the code below is for the metadata \"\"\"\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save the tflite model\n",
    "open('model.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "# create a label file for the tflite model\n",
    "labels = '\\n'.join(sorted(train_dataset.class_indices.keys()))\n",
    "with open('labels.txt', 'w') as f:\n",
    "    f.write(labels)\n",
    "\n",
    "\n",
    "# metadata for the tflite model\n",
    "model_file = 'model.tflite'\n",
    "label_file = 'labels.txt'\n",
    "model_type = 'image.classification'\n",
    "name = 'Image Classification'\n",
    "description = 'Identify if the image is good or bad'\n",
    "version = 'v1'\n",
    "image_width = 200\n",
    "image_height = 200\n",
    "image_min = 0\n",
    "image_max = 255\n",
    "mean = [0]\n",
    "std = [255]\n",
    "is_quantized = False\n",
    "model_author = 'MVP'\n",
    "model_license = 'Apache License. Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.'\n",
    "\n",
    "# create the metadata\n",
    "model_meta = _metadata_fb.ModelMetadataT()\n",
    "model_meta.name = name\n",
    "model_meta.description = description\n",
    "model_meta.version = version\n",
    "model_meta.author = model_author\n",
    "model_meta.license = model_license\n",
    "\n",
    "# # create the associated file\n",
    "# associated\n",
    "\n",
    "# create the input image\n",
    "input_meta = _metadata_fb.TensorMetadataT()\n",
    "input_meta.name = 'input_image'\n",
    "input_meta.description = ('Input image to be classified.')\n",
    "input_meta.content = _metadata_fb.ContentT()\n",
    "input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
    "input_meta.content.contentProperties.colorSpace = (\n",
    "    _metadata_fb.ColorSpaceType.RGB)\n",
    "input_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.ImageProperties)\n",
    "input_normalization = _metadata_fb.ProcessUnitT()\n",
    "input_normalization.optionsType = (\n",
    "    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "input_normalization.options.mean = mean\n",
    "input_normalization.options.std = std\n",
    "input_meta.processUnits = [input_normalization]\n",
    "input_stats = _metadata_fb.StatsT()\n",
    "input_stats.max = [image_max]\n",
    "input_stats.min = [image_min]\n",
    "input_meta.stats = input_stats\n",
    "\n",
    "# create the output\n",
    "output_meta = _metadata_fb.TensorMetadataT()\n",
    "output_meta.name = 'probability'\n",
    "output_meta.description = ('Probabilities of the label respectively.')\n",
    "output_meta.content = _metadata_fb.ContentT()\n",
    "output_meta.content.contentProperties = _metadata_fb.FeaturePropertiesT()\n",
    "output_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_meta.stats = _metadata_fb.StatsT()\n",
    "output_meta.stats.max = [1.0]\n",
    "output_meta.stats.min = [0.0]\n",
    "\n",
    "# create the subgraph\n",
    "subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [output_meta]\n",
    "subgraph.inputTensorNames = ['input_1']\n",
    "subgraph.outputTensorNames = ['dense_1']\n",
    "model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "# create the buffer\n",
    "b = flatbuffers.Builder(0)\n",
    "b.Finish(\n",
    "    model_meta.Pack(b),\n",
    "    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = b.Output()\n",
    "\n",
    "# create the populator\n",
    "populator = _metadata.MetadataPopulator.with_model_file(model_file)\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.load_associated_files([label_file])\n",
    "populator.populate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata json saved to:  metadata.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save the metadata to a file. JSON\"\"\"\n",
    "# visualize the metadata\n",
    "displayer = _metadata.MetadataDisplayer.with_model_file(model_file)\n",
    "export_json_file = 'metadata.json'\n",
    "json_file = displayer.get_metadata_json()\n",
    "with open(export_json_file, 'w') as f:\n",
    "    f.write(json_file)\n",
    "print('Metadata json saved to: ', export_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everything above should be what you need to create a tflite model with metadata. Everything below is old code which is not needed.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Everything above should be what you need to create a tflite model with metadata. Everything below is old code which is not needed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inludeed in the above code already'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Inludeed in the above code already\"\"\"\n",
    "# # compile the model\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#                 optimizer=RMSprop(lr=0.001),\n",
    "#                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't need this\\n\\n    This is just to visualize the accuracy and loss of the model\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Inludeed in the above code already\"\"\"\n",
    "# # train the model\n",
    "# history = model.fit(train_dataset,\n",
    "#                     steps_per_epoch=19, \n",
    "#                     epochs=10,\n",
    "#                     validation_data=validation_dataset)\n",
    "\n",
    "\"\"\"Don't need this\n",
    "\n",
    "    This is just to visualize the accuracy and loss of the model\"\"\"\n",
    "# # plot the accuracy and loss into a graph\n",
    "# acc = history.history['accuracy'] # accuracy\n",
    "# val_acc = history.history['val_accuracy'] # validation accuracy\n",
    "# loss = history.history['loss'] # loss\n",
    "# val_loss = history.history['val_loss'] # validation loss\n",
    "\n",
    "# # note: to increase the accuracy, we can \n",
    "# # 1. get more data\n",
    "# # 2. add more layers \n",
    "# # 3. change image size, not too small or too large\n",
    "# # 4. increase the number of epochs\n",
    "\n",
    "# epochs = range(len(acc))\n",
    "\n",
    "# plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend(loc=0)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't need this\\n\\n    This is just to visualize the accuracy and loss of the model\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Don't need this\n",
    "\n",
    "    This is just to visualize the accuracy and loss of the model\"\"\"\n",
    "# plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend(loc=0)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't need this\\n\\n    uncomment the whole thing if you want to see some of the output of the test dataset\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Don't need this\n",
    "\n",
    "    uncomment the whole thing if you want to see some of the output of the test dataset\"\"\"\n",
    "# # predict the image in the custom-ava-downloader/validation_image folder with the model\n",
    "# # print the prediction result\n",
    "# # if the prediction result is 1, then the image is a good image\n",
    "# # if the prediction result is 0, then the image is a bad image\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "# # read JSON file, for comparsion later on with prediction result\n",
    "# with open('/mnt/c/Users/joe19/Desktop/EECS441/custom-ava-downloader/image_data.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "# # loop through the images in the custom-ava-downloader/validation_image folder and predict the image\n",
    "# for image_name in os.listdir('custom-ava-downloader/Testing'):\n",
    "\n",
    "#     # resize this image \n",
    "#     test_image = image.load_img('custom-ava-downloader/Testing/' + image_name, target_size = (200, 200))\n",
    "\n",
    "#     # print the image \n",
    "#     plt.imshow(test_image)\n",
    "#     plt.show()\n",
    "\n",
    "#     # convert the image to an array\n",
    "#     test_image = image.img_to_array(test_image)\n",
    "\n",
    "#     # expand the dimension of the image\n",
    "#     test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "#     # predict the image\n",
    "#     result = model.predict(test_image)\n",
    "#     if result[0][0] == 1:\n",
    "#         prediction = 'good image'\n",
    "#     else:\n",
    "#         prediction = 'bad image'\n",
    "#     print(image_name + \" is a \" + prediction + \" with a prediction result of \" + str(result[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the model to a tflite model\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # save the tflite model\n",
    "# open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
